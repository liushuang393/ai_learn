{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机（Perceptron）是最基本的神经网络之一，也是神经网络的起点。它主要用于二分类问题，并且基于线性可分数据的假设。以下是感知机的核心概念和实现方式：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 感知机基本概念\n",
    "感知机是一种**前馈神经网络**，具有以下关键部分：\n",
    "- **输入层**：用于接收特征数据（每个特征对应一个输入神经元）。\n",
    "- **权重（Weights）**：每个输入特征都有一个对应的权重，表示其重要性。\n",
    "- **偏置（Bias）**：用于调整分类的决策边界。\n",
    "- **激活函数**：用于决定感知机的输出，一般是**阶跃函数（Step Function）**。\n",
    "\n",
    "数学表达：\n",
    "\\[\n",
    "y = f(WX + b)\n",
    "\\]\n",
    "其中：\n",
    "- \\( W \\) 是权重向量\n",
    "- \\( X \\) 是输入向量\n",
    "- \\( b \\) 是偏置\n",
    "- \\( f \\) 是激活函数（通常是符号函数 Sign，如 \\(\\text{sign}(x)\\)）\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 感知机算法流程\n",
    "1. **初始化**：随机初始化权重 \\( W \\) 和偏置 \\( b \\)。\n",
    "2. **前向传播**：\n",
    "   - 计算加权输入 \\( z = WX + b \\)。\n",
    "   - 通过激活函数计算输出 \\( y \\)。\n",
    "3. **计算误差**：\n",
    "   - 误差 \\( e = y_{\\text{真实}} - y_{\\text{预测}} \\)。\n",
    "4. **更新权重（基于梯度下降）**：\n",
    "   - 更新规则：\n",
    "     \\[\n",
    "     W = W + \\eta \\cdot e \\cdot X\n",
    "     \\]\n",
    "     \\[\n",
    "     b = b + \\eta \\cdot e\n",
    "     \\]\n",
    "   其中，\\( \\eta \\) 是学习率，决定每次更新的步长。\n",
    "5. **重复**步骤 2-4，直到误差足够小或达到最大迭代次数。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Python 实现一个简单的感知机\n",
    "以下是一个用于二分类（如 AND、OR、XOR 任务）的简单感知机实现：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
    "        self.lr = lr  # 学习率\n",
    "        self.epochs = epochs  # 训练轮次\n",
    "        self.weights = np.random.randn(input_size)  # 初始化权重\n",
    "        self.bias = np.random.randn()  # 初始化偏置\n",
    "    \n",
    "    def activation(self, x):\n",
    "        \"\"\"阶跃激活函数\"\"\"\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        return self.activation(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"训练感知机\"\"\"\n",
    "        for _ in range(self.epochs):\n",
    "            for X, y in zip(X_train, y_train):\n",
    "                y_pred = self.predict(X)\n",
    "                error = y - y_pred\n",
    "                self.weights += self.lr * error * X  # 更新权重\n",
    "                self.bias += self.lr * error  # 更新偏置\n",
    "\n",
    "# 训练数据（AND 逻辑）\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 0, 0, 1])  # AND 逻辑\n",
    "\n",
    "# 创建感知机并训练\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "for X in X_train:\n",
    "    print(f\"输入: {X}, 预测: {perceptron.predict(X)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 感知机的局限性\n",
    "- **仅适用于线性可分问题**：例如 AND 和 OR 逻辑可以被感知机解决，但 XOR 不能，因为 XOR 数据是非线性可分的。\n",
    "- **单层感知机无法学习复杂特征**：多层感知机（MLP）使用**非线性激活函数（如 ReLU、Sigmoid）**，能够学习复杂的映射关系。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 进阶：多层感知机（MLP）\n",
    "如果要解决 XOR 这样**非线性可分问题**，可以使用**多层感知机（MLP）**，即：\n",
    "- **增加隐藏层**\n",
    "- **使用非线性激活函数**\n",
    "- **利用反向传播算法（Backpropagation）训练网络**\n",
    "\n",
    "MLP 是深度学习的基础，可以用 `TensorFlow` 或 `PyTorch` 轻松实现：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # 2输入 -> 4隐藏\n",
    "        self.output = nn.Linear(4, 1)  # 4隐藏 -> 1输出\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        return self.activation(self.output(x))\n",
    "\n",
    "# 训练 XOR 数据集\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "model = MLP()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练模型\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 预测 XOR 结果\n",
    "for X_test in X:\n",
    "    print(f\"输入: {X_test.numpy()}, 预测: {model(X_test).item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 总结\n",
    "- **感知机是神经网络的基础**，适用于线性可分问题。\n",
    "- **多层感知机（MLP）** 扩展了感知机，能够处理更复杂的任务。\n",
    "- **现代深度学习框架（如 PyTorch、TensorFlow）** 让 MLP 的实现更加高效和便捷。\n",
    "\n",
    "如果你是**初学者**，建议先理解感知机的基本原理，再尝试 MLP 解决更复杂的问题，比如 XOR 逻辑问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个用于二分类（如 AND、OR、XOR 任务）的简单感知机实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: [0 0], 预测: 0\n",
      "输入: [0 1], 预测: 0\n",
      "输入: [1 0], 预测: 0\n",
      "输入: [1 1], 预测: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
    "        self.lr = lr  # 学习率\n",
    "        self.epochs = epochs  # 训练轮次\n",
    "        self.weights = np.random.randn(input_size)  # 初始化权重\n",
    "        self.bias = np.random.randn()  # 初始化偏置\n",
    "    \n",
    "    def activation(self, x):\n",
    "        \"\"\"阶跃激活函数\"\"\"\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        return self.activation(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"训练感知机\"\"\"\n",
    "        for _ in range(self.epochs):\n",
    "            for X, y in zip(X_train, y_train):\n",
    "                y_pred = self.predict(X)\n",
    "                error = y - y_pred\n",
    "                self.weights += self.lr * error * X  # 更新权重\n",
    "                self.bias += self.lr * error  # 更新偏置\n",
    "\n",
    "# 训练数据（AND 逻辑）\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 0, 0, 1])  # AND 逻辑\n",
    "\n",
    "# 创建感知机并训练\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "for X in X_train:\n",
    "    print(f\"输入: {X}, 预测: {perceptron.predict(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进阶：多层感知机（MLP）\n",
    "如果要解决 XOR 这样非线性可分问题，可以使用多层感知机（MLP），即：\n",
    "\n",
    "增加隐藏层\n",
    "使用非线性激活函数\n",
    "利用反向传播算法（Backpropagation）训练网络\n",
    "MLP 是深度学习的基础，可以用 TensorFlow 或 PyTorch 轻松实现："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现原理\n",
    "XOR问题是一个非线性可分问题，无法用单层感知机解决。多层感知机通过引入隐藏层和激活函数，能够处理这种非线性问题。\n",
    "Sigmoid激活函数将输出映射到(0, 1)区间，适用于二分类问题。\n",
    "二元交叉熵损失函数衡量预测值与真实值之间的差距，适用于二分类问题。\n",
    "随机梯度下降优化器通过迭代更新模型参数，最小化损失函数。\n",
    "用途\n",
    "该代码展示了如何使用PyTorch构建和训练一个简单的多层感知机来解决XOR问题，可以作为神经网络入门的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现了一个多层感知机（MLP）来训练和预测XOR逻辑门。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: [0. 0.], 预测: 0.4874\n",
      "输入: [0. 1.], 预测: 0.4909\n",
      "输入: [1. 0.], 预测: 0.5092\n",
      "输入: [1. 1.], 预测: 0.5105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # 2输入 -> 4隐藏\n",
    "        self.output = nn.Linear(4, 1)  # 4隐藏 -> 1输出\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        return self.activation(self.output(x))\n",
    "\n",
    "# 训练 XOR 数据集\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "model = MLP()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练模型\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 预测 XOR 结果\n",
    "for X_test in X:\n",
    "    print(f\"输入: {X_test.numpy()}, 预测: {model(X_test).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **学习率（η, Learning Rate）详解**\n",
    "学习率（\\( \\eta \\)）是**神经网络训练中的超参数**，用于**控制模型更新权重的步长**。它是**梯度下降**算法中的关键参数，决定了每次参数调整的幅度。\n",
    "\n",
    "---\n",
    "\n",
    "## **1. 学习率的作用**\n",
    "在**梯度下降（Gradient Descent）**过程中，模型的**权重**（\\( W \\)）是通过**损失函数（Loss Function）**的梯度来更新的，公式如下：\n",
    "\n",
    "\\[\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( W_{\\text{new}} \\) 是更新后的权重\n",
    "- \\( W_{\\text{old}} \\) 是当前的权重\n",
    "- \\( \\eta \\) 是**学习率**\n",
    "- \\( \\frac{\\partial L}{\\partial W} \\) 是损失函数关于权重的梯度（导数）\n",
    "\n",
    "学习率决定了**权重更新的速度**：\n",
    "- **学习率过大（\\( \\eta \\) 大）**：权重更新过快，可能导致**错过最优解**或**发散**。\n",
    "- **学习率过小（\\( \\eta \\) 小）**：权重更新过慢，可能**收敛速度慢**，甚至陷入局部最优。\n",
    "\n",
    "---\n",
    "\n",
    "## **2. 选择合适的学习率**\n",
    "### **（1）学习率过大：容易发散**\n",
    "如果 \\( \\eta \\) 过大，模型在优化过程中可能会**跳过最优解**，导致震荡或发散：\n",
    "\n",
    "| 学习率 \\( \\eta \\) | 结果 |\n",
    "|-----------------|------------------|\n",
    "| 过大 (如 1.0)  | 可能会跳过最优点，导致模型发散 |\n",
    "| 适中 (如 0.01) | 能够平稳收敛到最优解 |\n",
    "| 过小 (如 0.0001) | 训练太慢，可能停留在局部最优 |\n",
    "\n",
    "示意图：\n",
    "```plaintext\n",
    "(学习率过大)       (合适)        (学习率过小)\n",
    "   ----->         ----->            --->\n",
    "     ----->      ----->             --->\n",
    "```\n",
    "\n",
    "### **（2）学习率过小：收敛速度慢**\n",
    "如果 \\( \\eta \\) 过小，模型更新的幅度太小，训练过程可能会变得**极其缓慢**，甚至**卡在局部最优点**，无法到达真正的最优解。\n",
    "\n",
    "---\n",
    "\n",
    "## **3. 如何调整学习率**\n",
    "为了找到**最佳学习率**，可以使用以下方法：\n",
    "\n",
    "### **（1）手动调整**\n",
    "最常见的方法是**手动调试学习率**：\n",
    "- 从 **0.1~0.0001** 之间尝试不同的值。\n",
    "- 如果**收敛慢**，可以**增大学习率**（如从 `0.001` 变为 `0.01`）。\n",
    "- 如果**发散**，需要**减小学习率**（如从 `0.1` 变为 `0.01`）。\n",
    "\n",
    "### **（2）学习率衰减（Learning Rate Decay）**\n",
    "在训练过程中逐渐降低学习率，保证前期快速收敛，后期稳定收敛：\n",
    "\\[\n",
    "\\eta_t = \\eta_0 \\times \\frac{1}{1 + k \\cdot t}\n",
    "\\]\n",
    "PyTorch 示例：\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "```\n",
    "- `step_size=10`：每 10 轮降低学习率\n",
    "- `gamma=0.5`：学习率每次乘以 `0.5`\n",
    "\n",
    "### **（3）自适应优化算法**\n",
    "一些优化算法可以**动态调整学习率**：\n",
    "- **Adam**（推荐）：自动调整每个参数的学习率。\n",
    "- **RMSprop**：适用于非平稳目标。\n",
    "- **Adagrad**：适用于稀疏数据。\n",
    "\n",
    "PyTorch 示例：\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. 代码示例：不同学习率的影响**\n",
    "以下代码比较了不同学习率的影响：\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# **簡単な回帰モデル**\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # y = Wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# **データセット（y = 2x + 3）**\n",
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = 2 * X + 3 + 0.2 * torch.randn(X.size())  # ノイズ付き\n",
    "\n",
    "# **異なる学習率を試す**\n",
    "learning_rates = [0.01, 0.1, 0.5]  # 3つの学習率\n",
    "loss_history = {lr: [] for lr in learning_rates}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = SimpleModel()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history[lr].append(loss.item())\n",
    "\n",
    "# **学習率の比較（可視化）**\n",
    "plt.figure(figsize=(8,6))\n",
    "for lr, losses in loss_history.items():\n",
    "    plt.plot(losses, label=f\"η = {lr}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"異なる学習率の影響\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. 结论**\n",
    "1. **学习率决定训练的快慢**\n",
    "   - **太大**（如 `0.5`）：跳动大，可能发散。\n",
    "   - **太小**（如 `0.0001`）：收敛慢，浪费计算资源。\n",
    "   - **合适的学习率**（如 `0.01`）：快速稳定收敛。\n",
    "\n",
    "2. **调整学习率的方法**\n",
    "   - **手动调节**：尝试 `0.01`、`0.001`、`0.0001` 等。\n",
    "   - **学习率衰减（Decay）**：随时间降低学习率。\n",
    "   - **使用自适应优化器**（Adam、RMSprop）。\n",
    "\n",
    "3. **PyTorch 实践**\n",
    "   - 选择 `torch.optim.Adam()` **自动调整学习率**，一般比 `SGD` 效果更好。\n",
    "   - 使用 `torch.optim.lr_scheduler.StepLR()` **动态调整学习率**，让训练更平稳。\n",
    "\n",
    "---\n",
    "\n",
    "如果你想进一步优化 **PyTorch 训练中的学习率策略**，或者有其他需要，可以告诉我！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
